# 深度学习

## 一、线性回归模型

**y=wx+b**

### 线性回归模型参数求解：

①穷举法

②最小二乘法

![](./img/0822/1.png)

![](./img/0822/2.png)

求极值，令偏导数为0

![](./img/0822/3.png)

**⭐③梯度下降法**

核心：找到正确的方向

![image-20250823155007428](./img/0822/4.png)

## 二、逻辑回归模型

逻辑回归算法的原理：

![image-20250823175710931](./img/0822/5.png)

![image-20250823175826547](./img/0822/6.png)

![image-20250823180845066](./img/0822/7.png)

(1)参数w更新（交叉熵损失函数）:

![image-20250823213700502](./img/0822/8.png)

（2）参数b更新：

![image-20250823214214768](./img/0822/9.png)

评价指标（P:postive,N:negative）：

![image-20250823215259499](./img/0822/10.png)

准确率：(TP+TN)/(TP+TN+FP+FN)

精确率：P = TP/(TP+FP)

召回率：召回=TP/(TP+FN)

F1值（精确率和召回率的调和均值）：2TP/(2TP+FP+FN)



## 三、全连接神经网络

神经元的数学表达式：

a = h(w*x+b)（h：为激活函数，非线性）

### 激活函数

作用：提供神经网络效率，神经网络隐藏层越深，效果越好

（1）Sigmoid函数

![image-20250826090700597](./img/0822/11.png)

（2）Tanh函数

![image-20250826091145136](./img/0822/12.png)

（3）ReLU函数

<img src="./img/0822/13.png" alt="image-20250826091546995" style="zoom:200%;" />

（4）Leaky ReLU函数

![image-20250826092025682](./img/0822/14.png)

### 损失函数

![image-20250826100156438](D:/postGrudate/postGrudate_study/img/0822/15.png)

![image-20250830172455536](./img/0827/4.png)

### 卷积层

**⭐输出特征图大小计算公式（有小数就向下取整）**：

![image-20250827101907631](./img/0822/16.png)

P：填充大小 S：步幅

### 池化层

最大池化：找最大值

![image-20250827103630787](./img/0822/17.png)

平均池化：找平均值

卷积神经网络整体结构：

![image-20250827104327082](./img/0822/18.png)

### LeNet和原理与实战

特征提取部分和全连接层

![image-20250828103413071](./img/0827/1.png)

### AlexNet

![image-20250828214154065](./img/0827/2.png)

![image-20250828214218276](./img/0827/3.png)

LRN正则化：对局部的值进行归一化操作，使其中比较大的值变得相对更大，增强了局部的对比度。

## 模型训练过程

#### 1、收集数据，数据预处理

#### 2、搭建模型

#### 3、开始训练：学习率，学习训练伦次等参数

##### 3.1、前向传播

##### 3.2、计算误差

##### 3.3、反向传播（梯度下降法）

##### 3.4、直到训练伦次结束

##### 3.5、产生model（w，b更新完毕）

#### 4、开始测试

#### 5、应用

## RNN循环神经网络

![image-20250904091430807](./img/0827/5.png)

![image-20250904095256772](./img/0827/6.png)

![image-20250904102028812](./img/0827/7.png)

## LSTM模型

LSTM（长短时记忆网络）是一种常用于处理序列数据的深度学习模型，与传统的 RNN（循环神经网络）相比，LSTM引入了三个门（ **输入门、遗忘门、输出门**，如下图所示）和一个 ***细胞状态\***（cell state），这些机制使得LSTM能够更好地处理序列中的长期依赖关系。

## GRU模型

### 一、定义

GRU（Gate Recurrent Unit）是循环神经网络（[RNN](https://so.csdn.net/so/search?q=RNN&spm=1001.2101.3001.7020)）的一种，可以解决RNN中不能长期记忆和反向传播中的梯度等问题，与LSTM的作用类似，不过比LSTM简单，容易进行训练。
